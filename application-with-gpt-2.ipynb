{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1435929,"sourceType":"datasetVersion","datasetId":841289},{"sourceId":2770793,"sourceType":"datasetVersion","datasetId":1690964}],"dockerImageVersionId":30146,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"5\"\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport Levenshtein\nimport cv2\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport torchvision\nimport PIL.Image as Image\nfrom torchvision.transforms import ToTensor, ToPILImage\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch\nfrom torch import nn\nfrom nltk.translate.bleu_score import corpus_bleu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-12-28T18:43:54.572589Z","iopub.execute_input":"2021-12-28T18:43:54.573347Z","iopub.status.idle":"2021-12-28T18:43:54.584251Z","shell.execute_reply.started":"2021-12-28T18:43:54.573308Z","shell.execute_reply":"2021-12-28T18:43:54.583349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Increase Batch size based on your hardware capacity, if you have powerful GPU try BS=128,256...\ntrain_path=r'../input/bird-species-classification-220-categories/Train'\ntest_path=r'../input/bird-species-classification-220-categories/Test'","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:43:55.810987Z","iopub.execute_input":"2021-12-28T18:43:55.811244Z","iopub.status.idle":"2021-12-28T18:43:55.815512Z","shell.execute_reply.started":"2021-12-28T18:43:55.811215Z","shell.execute_reply":"2021-12-28T18:43:55.814192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_class = {}\nimages_class = []\nwith os.scandir('../input/captions/text/') as entries:\n    for entry in entries:\n        \n       # with os.scandir(entry) as entries:\n       if os.path.isdir(entry):\n            caption_class[entry.name.split('.')[1]] = entry.name\n        #caption_class.append(entry.name.split('.')[1])\nwith os.scandir('../input/bird-species-classification-220-categories/Train') as entrie:\n    for entry in entrie:\n        images_class.append(entry.name)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:43:56.523007Z","iopub.execute_input":"2021-12-28T18:43:56.523488Z","iopub.status.idle":"2021-12-28T18:43:56.676705Z","shell.execute_reply.started":"2021-12-28T18:43:56.523449Z","shell.execute_reply":"2021-12-28T18:43:56.675373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(images_class))","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:43:57.411396Z","iopub.execute_input":"2021-12-28T18:43:57.41221Z","iopub.status.idle":"2021-12-28T18:43:57.417732Z","shell.execute_reply.started":"2021-12-28T18:43:57.412161Z","shell.execute_reply":"2021-12-28T18:43:57.417021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\npath_caption = '../input/captions/text/'\ntrain_path=r'../input/bird-species-classification-220-categories/Train'\ntest_path=r'../input/bird-species-classification-220-categories/Test'\nimages_paths = []\ncaptions_paths = []\nimages_test = []\ncaptions_test = []\nname_captin = []\ntest_cap = []\n#embedding_paths = []\nfor c_image in images_class:\n    for image_name in os.listdir(os.path.join(train_path,c_image)):\n        d=os.path.join(train_path,c_image)\n        image_path =os.path.join(d,image_name)\n        images_paths.append(image_path)\n        name_captin_file = image_name[:-4]+\".txt\"\n        d=os.path.join(path_caption,caption_class[c_image])\n        caption_path =os.path.join(d,name_captin_file)\n        captions_paths.append(caption_path)\nfor c_image in images_class:\n    for image_name in os.listdir(os.path.join(test_path,c_image)):\n        d=os.path.join(test_path,c_image)\n        image_path =os.path.join(d,image_name)\n        images_test.append(image_path)\n        #img = cv2.imread(image_path)\n        name_captin_file = image_name[:-4]+\".txt\"\n        d=os.path.join(path_caption,caption_class[c_image])\n        caption_path =os.path.join(d,name_captin_file)\n       # embedding_path = os.path.join(d,embedding_caption)\n        captions_test.append(caption_path)\n    ","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:43:58.209543Z","iopub.execute_input":"2021-12-28T18:43:58.210099Z","iopub.status.idle":"2021-12-28T18:44:01.300597Z","shell.execute_reply.started":"2021-12-28T18:43:58.210058Z","shell.execute_reply":"2021-12-28T18:44:01.299864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(images_paths),len(captions_paths))\nprint(len(images_test),len(captions_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:44:01.302131Z","iopub.execute_input":"2021-12-28T18:44:01.302606Z","iopub.status.idle":"2021-12-28T18:44:01.308945Z","shell.execute_reply.started":"2021-12-28T18:44:01.302567Z","shell.execute_reply":"2021-12-28T18:44:01.308176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:44:04.052899Z","iopub.execute_input":"2021-12-28T18:44:04.05315Z","iopub.status.idle":"2021-12-28T18:44:26.661351Z","shell.execute_reply.started":"2021-12-28T18:44:04.053123Z","shell.execute_reply":"2021-12-28T18:44:26.660524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:44:26.662996Z","iopub.execute_input":"2021-12-28T18:44:26.663271Z","iopub.status.idle":"2021-12-28T18:44:29.703273Z","shell.execute_reply.started":"2021-12-28T18:44:26.663234Z","shell.execute_reply":"2021-12-28T18:44:29.702547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s = captions_paths + captions_test\nmax_seq = 0\nfor i in s:\n    with open(i,'r') as file:\n        lines = file.read()\n        m = len(tokenizer.encode(lines))\n        if max_seq < m:\n            max_seq = m","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:44:31.511014Z","iopub.execute_input":"2021-12-28T18:44:31.511266Z","iopub.status.idle":"2021-12-28T18:46:10.219788Z","shell.execute_reply.started":"2021-12-28T18:44:31.511238Z","shell.execute_reply":"2021-12-28T18:46:10.219012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:10.221282Z","iopub.execute_input":"2021-12-28T18:46:10.221557Z","iopub.status.idle":"2021-12-28T18:46:10.229039Z","shell.execute_reply.started":"2021-12-28T18:46:10.221514Z","shell.execute_reply":"2021-12-28T18:46:10.228211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_to_tensor(path,max_len):\n    with open(path,'r') as file:\n        lines = file.read()\n        m = tokenizer.encode(lines)\n        m.append(0)\n        len_seq = len(m)\n        tensor = torch.LongTensor(F.pad(torch.LongTensor(m), pad=(0, (max_len+1 )- len_seq) , mode='constant', value=0)) \n    return tensor,len_seq\n\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(256),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.5], [0.5]),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,captions_paths,max_len,transform=None):\n        self.images_paths = images_paths\n        self.captions_paths = captions_paths\n        self.max_len = max_len \n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        tensor_caption,len_seq = text_to_tensor(self.captions_paths[idx],self.max_len)\n        #seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long(\n        img_transformed = self.transform(img)   \n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed,tensor_caption,len_seq","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:13.235024Z","iopub.execute_input":"2021-12-28T18:46:13.23529Z","iopub.status.idle":"2021-12-28T18:46:13.247254Z","shell.execute_reply.started":"2021-12-28T18:46:13.235259Z","shell.execute_reply":"2021-12-28T18:46:13.246535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(images_paths),len(captions_paths))\nprint(len(images_test),len(captions_test))","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:15.231627Z","iopub.execute_input":"2021-12-28T18:46:15.232218Z","iopub.status.idle":"2021-12-28T18:46:15.253577Z","shell.execute_reply.started":"2021-12-28T18:46:15.23217Z","shell.execute_reply":"2021-12-28T18:46:15.25111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_paths  = images_paths[:9408]\ncaptions_paths = captions_paths[:9408]\ntest_images = images_test[:2368] \ntest_captions = captions_test[:2368] \ntrain_data = dataset(images_paths,captions_paths,max_seq,transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=32, shuffle=True)\ntest_data = dataset(test_images,test_captions,max_seq,transform=train_transforms)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=32, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:16.868007Z","iopub.execute_input":"2021-12-28T18:46:16.868256Z","iopub.status.idle":"2021-12-28T18:46:16.875134Z","shell.execute_reply.started":"2021-12-28T18:46:16.868227Z","shell.execute_reply":"2021-12-28T18:46:16.874007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data))\nprint(len(train_loader))\nprint(len(test_data))\nprint(len(test_loader))","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:19.106384Z","iopub.execute_input":"2021-12-28T18:46:19.106634Z","iopub.status.idle":"2021-12-28T18:46:19.114108Z","shell.execute_reply.started":"2021-12-28T18:46:19.106607Z","shell.execute_reply":"2021-12-28T18:46:19.113173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\n\nclass NetG(nn.Module):\n    def __init__(self, ngf=64, nz=100):\n        super(NetG, self).__init__()\n        self.ngf = ngf\n\n        # layer1输入的是一个100x1x1的随机噪声, 输出尺寸(ngf*8)x4x4\n        self.fc = nn.Linear(nz, ngf*8*4*4)\n        self.block0 = G_Block(ngf * 8, ngf * 8)#4x4\n        self.block1 = G_Block(ngf * 8, ngf * 8)#4x4\n        self.block2 = G_Block(ngf * 8, ngf * 8)#8x8\n        self.block3 = G_Block(ngf * 8, ngf * 8)#16x16\n        self.block4 = G_Block(ngf * 8, ngf * 4)#32x32\n        self.block5 = G_Block(ngf * 4, ngf * 2)#64x64\n        self.block6 = G_Block(ngf * 2, ngf * 1)#128x128\n\n        self.conv_img = nn.Sequential(\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ngf, 3, 3, 1, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x, c):\n\n        out = self.fc(x)\n        out = out.view(x.size(0), 8*self.ngf, 4, 4)\n        out = self.block0(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block1(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block2(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block3(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block4(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block5(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block6(out,c)\n\n        out = self.conv_img(out)\n\n        return out\n\n\n\nclass G_Block(nn.Module):\n\n    def __init__(self, in_ch, out_ch):\n        super(G_Block, self).__init__()\n\n        self.learnable_sc = in_ch != out_ch \n        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n        self.affine0 = affine(in_ch)\n        self.affine1 = affine(in_ch)\n        self.affine2 = affine(out_ch)\n        self.affine3 = affine(out_ch)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        if self.learnable_sc:\n            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n\n    def forward(self, x, y=None):\n        return self.shortcut(x) + self.gamma * self.residual(x, y)\n\n    def shortcut(self, x):\n        if self.learnable_sc:\n            x = self.c_sc(x)\n        return x\n\n    def residual(self, x, y=None):\n        h = self.affine0(x, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.affine1(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.c1(h)\n        \n        h = self.affine2(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.affine3(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        return self.c2(h)\n\n\n\nclass affine(nn.Module):\n\n    def __init__(self, num_features):\n        super(affine, self).__init__()\n\n        self.fc_gamma = nn.Sequential(OrderedDict([\n            ('linear1',nn.Linear(256, 256)),\n            ('relu1',nn.ReLU(inplace=True)),\n            ('linear2',nn.Linear(256, num_features)),\n            ]))\n        self.fc_beta = nn.Sequential(OrderedDict([\n            ('linear1',nn.Linear(256, 256)),\n            ('relu1',nn.ReLU(inplace=True)),\n            ('linear2',nn.Linear(256, num_features)),\n            ]))\n        self._initialize()\n\n    def _initialize(self):\n        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n\n    def forward(self, x, y=None):\n\n        weight = self.fc_gamma(y)\n        bias = self.fc_beta(y)        \n\n        if weight.dim() == 1:\n            weight = weight.unsqueeze(0)\n        if bias.dim() == 1:\n            bias = bias.unsqueeze(0)\n\n        size = x.size()\n        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n        return weight * x + bias\n\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n\n        self.joint_conv = nn.Sequential(\n            nn.Conv2d(ndf * 16+256, ndf * 2, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n        )\n\n    def forward(self, out, y):\n        \n        y = y.view(-1, 256, 1, 1)\n        y = y.repeat(1, 1, 4, 4)\n        h_c_code = torch.cat((out, y), 1)\n        out = self.joint_conv(h_c_code)\n        return out\n\n\n\n\n\n# 定义鉴别器网络D\nclass NetD(nn.Module):\n    def __init__(self, ndf):\n        super(NetD, self).__init__()\n\n        self.conv_img = nn.Conv2d(3, ndf, 3, 1, 1)#128\n        self.block0 = resD(ndf * 1, ndf * 2)#64\n        self.block1 = resD(ndf * 2, ndf * 4)#32\n        self.block2 = resD(ndf * 4, ndf * 8)#16\n        self.block3 = resD(ndf * 8, ndf * 16)#8\n        self.block4 = resD(ndf * 16, ndf * 16)#4\n        self.block5 = resD(ndf * 16, ndf * 16)#4\n\n        self.COND_DNET = D_GET_LOGITS(ndf)\n\n    def forward(self,x):\n\n        out = self.conv_img(x)\n        out = self.block0(out)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n\n        return out\n\n\n\n\nclass resD(nn.Module):\n    def __init__(self, fin, fout, downsample=True):\n        super().__init__()\n        self.downsample = downsample\n        self.learned_shortcut = (fin != fout)\n        self.conv_r = nn.Sequential(\n            nn.Conv2d(fin, fout, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(fout, fout, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv_s = nn.Conv2d(fin,fout, 1, stride=1, padding=0)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, c=None):\n        return self.shortcut(x)+self.gamma*self.residual(x)\n\n    def shortcut(self, x):\n        if self.learned_shortcut:\n            x = self.conv_s(x)\n        if self.downsample:\n            return F.avg_pool2d(x, 2)\n        return x\n\n    def residual(self, x):\n        return self.conv_r(x)\n\n\n\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-02T12:05:06.4772Z","iopub.status.idle":"2021-11-02T12:05:06.478015Z","shell.execute_reply.started":"2021-11-02T12:05:06.477779Z","shell.execute_reply":"2021-11-02T12:05:06.477806Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:23.734411Z","iopub.execute_input":"2021-12-28T18:46:23.734676Z","iopub.status.idle":"2021-12-28T18:46:23.740213Z","shell.execute_reply.started":"2021-12-28T18:46:23.734635Z","shell.execute_reply":"2021-12-28T18:46:23.739326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\n\nclass NetG(nn.Module):\n    def __init__(self, ngf=64, nz=100):\n        super(NetG, self).__init__()\n        self.ngf = ngf\n        self.cr = nn.Linear(768,256)\n        self.fc = nn.Linear(nz, ngf*8*4*4)\n        self.block0 = G_Block(ngf * 8, ngf * 8)#4x4\n        self.block1 = G_Block(ngf * 8, ngf * 8)#4x4\n        self.block2 = G_Block(ngf * 8, ngf * 8)#8x8\n        self.block3 = G_Block(ngf * 8, ngf * 8)#16x16\n        self.block4 = G_Block(ngf * 8, ngf * 4)#32x32\n        self.block5 = G_Block(ngf * 4, ngf * 2)#64x64\n        self.block6 = G_Block(ngf * 2, ngf * 1)#128x128\n\n        self.conv_img = nn.Sequential(\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ngf, 3, 3, 1, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x, c):\n\n        out = self.fc(x)\n        out = out.view(x.size(0), 8*self.ngf, 4, 4)\n        c = self.cr(c)\n        out = self.block0(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block1(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block2(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block3(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block4(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block5(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block6(out,c)\n\n        out = self.conv_img(out)\n\n        return out\n\n\n\nclass G_Block(nn.Module):\n\n    def __init__(self, in_ch, out_ch):\n        super(G_Block, self).__init__()\n\n        self.learnable_sc = in_ch != out_ch \n        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n        self.affine0 = affine(in_ch)\n        self.affine1 = affine(in_ch)\n        self.affine2 = affine(out_ch)\n        self.affine3 = affine(out_ch)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        if self.learnable_sc:\n            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n\n    def forward(self, x, y=None):\n        return self.shortcut(x) + self.gamma * self.residual(x, y)\n\n    def shortcut(self, x):\n        if self.learnable_sc:\n            x = self.c_sc(x)\n        return x\n\n    def residual(self, x, y=None):\n        h = self.affine0(x, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.affine1(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.c1(h)\n        \n        h = self.affine2(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.affine3(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        return self.c2(h)\n\n\n\nclass affine(nn.Module):\n\n    def __init__(self, num_features):\n        super(affine, self).__init__()\n\n        self.fc_gamma = nn.Sequential(OrderedDict([\n            ('linear1',nn.Linear(256, 256)),\n            ('relu1',nn.ReLU(inplace=True)),\n            ('linear2',nn.Linear(256, num_features)),\n            ]))\n        self.fc_beta = nn.Sequential(OrderedDict([\n            ('linear1',nn.Linear(256, 256)),\n            ('relu1',nn.ReLU(inplace=True)),\n            ('linear2',nn.Linear(256, num_features)),\n            ]))\n        self._initialize()\n\n    def _initialize(self):\n        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n\n    def forward(self, x, y=None):\n\n        weight = self.fc_gamma(y)\n        bias = self.fc_beta(y)        \n\n        if weight.dim() == 1:\n            weight = weight.unsqueeze(0)\n        if bias.dim() == 1:\n            bias = bias.unsqueeze(0)\n\n        size = x.size()\n        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n        return weight * x + bias\n\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.kl = nn.Linear(768,256)\n        self.joint_conv = nn.Sequential(\n            nn.Conv2d(ndf * 16+256, ndf * 2, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n        )\n\n    def forward(self, out, y):\n        y = self.kl(y)\n        y = y.view(-1, 256, 1, 1)\n        y = y.repeat(1, 1, 4, 4)\n        h_c_code = torch.cat((out, y), 1)\n        out = self.joint_conv(h_c_code)\n        return out\n\n\n\n\n\nclass NetD(nn.Module):\n    def __init__(self, ndf):\n        super(NetD, self).__init__()\n        self.conv_img = nn.Conv2d(3, ndf, 3, 1, 1)#128\n        self.block0 = resD(ndf * 1, ndf * 2)#64\n        self.block1 = resD(ndf * 2, ndf * 4)#32\n        self.block2 = resD(ndf * 4, ndf * 8)#16\n        self.block3 = resD(ndf * 8, ndf * 16)#8\n        self.block4 = resD(ndf * 16, ndf * 16)#4\n        self.block5 = resD(ndf * 16, ndf * 16)#4\n\n        self.COND_DNET = D_GET_LOGITS(ndf)\n\n    def forward(self,x):\n\n        out = self.conv_img(x)\n        out = self.block0(out)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n\n        return out\n\n\n\n\nclass resD(nn.Module):\n    def __init__(self, fin, fout, downsample=True):\n        super().__init__()\n        self.downsample = downsample\n        self.learned_shortcut = (fin != fout)\n        self.conv_r = nn.Sequential(\n            nn.Conv2d(fin, fout, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(fout, fout, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv_s = nn.Conv2d(fin,fout, 1, stride=1, padding=0)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, c=None):\n        return self.shortcut(x)+self.gamma*self.residual(x)\n\n    def shortcut(self, x):\n        if self.learned_shortcut:\n            x = self.conv_s(x)\n        if self.downsample:\n            return F.avg_pool2d(x, 2)\n        return x\n\n    def residual(self, x):\n        return self.conv_r(x)","metadata":{"execution":{"iopub.status.busy":"2021-12-28T18:46:28.175647Z","iopub.execute_input":"2021-12-28T18:46:28.176073Z","iopub.status.idle":"2021-12-28T18:46:28.244292Z","shell.execute_reply.started":"2021-12-28T18:46:28.176036Z","shell.execute_reply":"2021-12-28T18:46:28.243444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### device = \"cuda\"\nimport torchvision.utils as vutils\nimport random\nbatch_size = 32\nrandom.seed(100)\nnp.random.seed(100)\ntorch.manual_seed(100)\ntorch.cuda.manual_seed_all(100)\nCUDA_LAUNCH_BLOCKING=1\nnetG = NetG(batch_size, 100).to(device)\nnetD = NetD(batch_size).to(device)\noptimizerG = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\noptimizerD = torch.optim.Adam(netD.parameters(), lr=0.0004, betas=(0.0, 0.9))\ncheckpoint_G = torch.load('../input/modul-bert-490/netG_vBE490.pt')\nnetG.load_state_dict(checkpoint_G['model_state_dict'])\noptimizerG.load_state_dict(checkpoint_G['optimizer_state_dict'])\nepoch_ = checkpoint_G['epoch']\n\ncheckpoint_D = torch.load('../input/modul-bert-490/netD_vBE490.pt')\nnetD.load_state_dict(checkpoint_D['model_state_dict'])\noptimizerD.load_state_dict(checkpoint_D['optimizer_state_dict'])\nnetG.eval()\nnetD.eval()","metadata":{"execution":{"iopub.status.busy":"2021-11-02T22:04:03.159213Z","iopub.execute_input":"2021-11-02T22:04:03.159701Z","iopub.status.idle":"2021-11-02T22:04:03.458623Z","shell.execute_reply.started":"2021-11-02T22:04:03.159663Z","shell.execute_reply":"2021-11-02T22:04:03.457763Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epoch_ = 0","metadata":{"execution":{"iopub.status.busy":"2021-11-02T22:04:29.419234Z","iopub.execute_input":"2021-11-02T22:04:29.4199Z","iopub.status.idle":"2021-11-02T22:04:29.422991Z","shell.execute_reply.started":"2021-11-02T22:04:29.419862Z","shell.execute_reply":"2021-11-02T22:04:29.42233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from miscc.utils import mkdir_p\n#from miscc.config import cfg, cfg_from_file\n\nfor epoch in range(epoch_ +1,epoch_ +41):\n    i = 0\n    for images,captions,cap_lens in train_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        #print(images.size())\n        with torch.no_grad():\n            last_hidden_states = model(captions.to(device))\n            sent_emb = last_hidden_states[0][:,-1,:].detach().to(device)\n        noise = torch.randn(batch_size, 100)\n        noise=noise.to(device)\n        fake = netG(noise,sent_emb.to(device))  \n            #imgs=images[0].to(device)\n        real_features = netD(images.to(device))\n        output = netD.COND_DNET(real_features,sent_emb.to(device))\n        errD_real = torch.nn.ReLU()(1.0 - output).mean()\n        output = netD.COND_DNET(real_features[:(batch_size - 1)].to(device), sent_emb[1:batch_size].to(device))\n        errD_mismatch = torch.nn.ReLU()(1.0 + output).mean()\n            #print(\"errD_mismatch :\",errD_mismatch.size())\n            # synthesize fake images\n        fake_features = netD(fake.detach()) \n        errD_fake = netD.COND_DNET(fake_features.to(device),sent_emb.to(device))\n        errD_fake = torch.nn.ReLU()(1.0 + errD_fake).mean()          \n        errD = errD_real + (errD_fake + errD_mismatch)/2.0\n            #print(\"errD : \",errD)\n        optimizerD.zero_grad()\n        optimizerG.zero_grad()\n        errD.backward()\n        optimizerD.step()\n            #MA-GP\n        interpolated = (images.data).requires_grad_()\n        sent_inter = (sent_emb.data).requires_grad_()\n        features = netD(interpolated.to(device))\n        out = netD.COND_DNET(features,sent_inter)\n        grads = torch.autograd.grad(outputs=out,\n                                    inputs=(interpolated,sent_inter),\n                                    grad_outputs=torch.ones(out.size()).to(device),\n                                    retain_graph=True,\n                                    create_graph=True,\n                                    only_inputs=True)\n        grad0 = grads[0].view(grads[0].size(0), -1).to(device)\n        grad1 = grads[1].view(grads[1].size(0), -1).to(device)\n        grad = torch.cat((grad0,grad1),dim=1)                        \n        grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n        d_loss_gp = torch.mean((grad_l2norm) ** 6)\n        d_loss = 2.0 * d_loss_gp\n        optimizerD.zero_grad()\n        optimizerG.zero_grad()\n        d_loss.backward()\n        optimizerD.step()\n        i += 1  \n            # update G\n        features = netD(fake)\n        output = netD.COND_DNET(features,sent_emb)\n        errG = - output.mean()\n        optimizerG.zero_grad()\n        optimizerD.zero_grad()\n        errG.backward()\n        optimizerG.step()\n        print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f'\n                % (epoch, 600, i, len(train_loader), errD.item(), errG.item()))\n        if (epoch+1) % 2 == 0 and i == 294:\n            print(\"save ................\")\n            vutils.save_image(fake.data,\n                        '%s/fake_samples_epoch_%03d.png' % ('./', epoch+1),\n                        normalize=True)\n            torch.save({\n            'model_state_dict': netD.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            },'./netD_v.pt')\n            \n            torch.save({\n            'epoch': epoch,\n            'model_state_dict': netG.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            },'./netG_v.pt')\n","metadata":{"execution":{"iopub.status.busy":"2021-11-02T22:04:32.775476Z","iopub.execute_input":"2021-11-02T22:04:32.775937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('./Output/')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-10-30T22:35:36.325551Z","iopub.execute_input":"2021-10-30T22:35:36.325893Z","iopub.status.idle":"2021-10-30T22:35:36.330164Z","shell.execute_reply.started":"2021-10-30T22:35:36.325863Z","shell.execute_reply":"2021-10-30T22:35:36.329059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#os.mkdir('./Output/')\nbatch_size = 32\nsave_dir = './Output/'\nd = 0\nt = 0\nfor i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n    for images,captions,cap_lens in test_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        with torch.no_grad():\n            last_hidden_states = model(captions.to(device))\n            sent_emb = last_hidden_states[0][:,-1,:].detach().to(device)\n            #sent_emb = sent_emb.transpose(0, 1).contiguous()\n            \n        #print(\"captions :\",captions.size())\n        #print(\"captions :\",captions[3])\n        #print(\"images :\",images.size())\n        #words_embs, sent_emb = text_encoder(captions.to(device),cap_lens.to(device),hidden)\n        #words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n        with torch.no_grad():\n            noise = torch.randn(batch_size, 100)\n            noise=noise.to(device)\n            fake_imgs = netG(noise,sent_emb)\n        print(\">>>>>>>>batch : \",d)\n        d += 1\n        for j in range(batch_size):\n            s_tmp = './Output/'\n            im = fake_imgs[j].data.cpu().numpy()\n                # [-1, 1] --> [0, 255]\n            #arab_cap = captions[j]\n            im = (im + 1.0) * 127.5\n            im = im.astype(np.uint8)\n            im = np.transpose(im, (1, 2, 0))\n            im = Image.fromarray(im)\n            #arab_cap = tokenizer.decode(arab_cap)\n            #cap_path = '%s_%3d.txt' % (s_tmp,t)\n            fullpath = '%s_%3d.png' % (s_tmp,t)\n            t += 1\n            im.save(fullpath)\n           # with open(cap_path,'w') as f:\n           #     f.write(arab_cap)\n                ","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:35:38.539087Z","iopub.execute_input":"2021-10-30T22:35:38.539425Z","iopub.status.idle":"2021-10-30T22:38:07.131435Z","shell.execute_reply.started":"2021-10-30T22:35:38.539394Z","shell.execute_reply":"2021-10-30T22:38:07.13064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r Output.zip ./Output","metadata":{"execution":{"iopub.status.busy":"2021-09-16T10:07:15.004123Z","iopub.execute_input":"2021-09-16T10:07:15.004456Z","iopub.status.idle":"2021-09-16T10:07:23.571739Z","shell.execute_reply.started":"2021-09-16T10:07:15.004427Z","shell.execute_reply":"2021-09-16T10:07:23.570622Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        \"\"\"Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, normalizes the input to the statistics the pretrained\n            Inception network expects\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        \"\"\"\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in \n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output \n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.upsample(x, size=(299, 299), mode='bilinear', align_corners=True)\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:24:50.269726Z","iopub.execute_input":"2021-09-16T12:24:50.270204Z","iopub.status.idle":"2021-09-16T12:24:50.317908Z","shell.execute_reply.started":"2021-09-16T12:24:50.270145Z","shell.execute_reply":"2021-09-16T12:24:50.317022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\nmodel_incp = InceptionV3([block_idx])\nmodel_incp=model_incp.cuda()","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:24:53.244263Z","iopub.execute_input":"2021-09-16T12:24:53.244591Z","iopub.status.idle":"2021-09-16T12:31:09.818774Z","shell.execute_reply.started":"2021-09-16T12:24:53.24456Z","shell.execute_reply":"2021-09-16T12:31:09.817739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n                    cuda=False):\n    model.eval()\n    act=np.empty((len(images), dims))\n    \n    if cuda:\n        batch=images.cuda()\n    else:\n        batch=images\n    pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n    if pred.size(2) != 1 or pred.size(3) != 1:\n        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n    \n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:31:14.335606Z","iopub.execute_input":"2021-09-16T12:31:14.335938Z","iopub.status.idle":"2021-09-16T12:31:14.345506Z","shell.execute_reply.started":"2021-09-16T12:31:14.335907Z","shell.execute_reply":"2021-09-16T12:31:14.344349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    \n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    \n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:31:16.038937Z","iopub.execute_input":"2021-09-16T12:31:16.03929Z","iopub.status.idle":"2021-09-16T12:31:16.050675Z","shell.execute_reply.started":"2021-09-16T12:31:16.039256Z","shell.execute_reply":"2021-09-16T12:31:16.049846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_fretchet(images_real,images_fake,model):\n     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n    \n     \"\"\"get fretched distance\"\"\"\n     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n     return fid_value","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:31:17.633648Z","iopub.execute_input":"2021-09-16T12:31:17.633956Z","iopub.status.idle":"2021-09-16T12:31:17.639411Z","shell.execute_reply.started":"2021-09-16T12:31:17.633927Z","shell.execute_reply":"2021-09-16T12:31:17.638333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transfor = transforms.Compose([\n        transforms.Resize(299),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        transforms.ToTensor(),\n        ])\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n\nclass dataset_fid(torch.utils.data.Dataset):\n\n    def __init__(self,images_test_paths,generated_images,transform=None):\n        self.images_test_paths = images_test_paths\n        self.generated_images = generated_images\n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_test_paths)\n        return self.filelength\n    \n    #load an one of images9\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img_test = torch.tensor(pil_loader(self.images_test_paths[idx].strip()))\n        img_gen = torch.tensor(pil_loader(self.generated_images[idx].strip()))\n        \n        img_test = self.transform(img_test) \n        img_test = self.transform(img_test)\n        return img_test,img_gen","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:46:21.55704Z","iopub.execute_input":"2021-10-30T22:46:21.557384Z","iopub.status.idle":"2021-10-30T22:46:21.568615Z","shell.execute_reply.started":"2021-10-30T22:46:21.557355Z","shell.execute_reply":"2021-10-30T22:46:21.567384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"./Output/\"\noutput_images = []\nwith os.scandir('./Output') as entrie:\n    for entry in entrie:\n        output_images.append(path+entry.name)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:49:53.661735Z","iopub.execute_input":"2021-10-30T22:49:53.662067Z","iopub.status.idle":"2021-10-30T22:49:53.670626Z","shell.execute_reply.started":"2021-10-30T22:49:53.662032Z","shell.execute_reply":"2021-10-30T22:49:53.669903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(output_images))\nprint(output_images[7])","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:49:56.827925Z","iopub.execute_input":"2021-10-30T22:49:56.828287Z","iopub.status.idle":"2021-10-30T22:49:56.833055Z","shell.execute_reply.started":"2021-10-30T22:49:56.828256Z","shell.execute_reply":"2021-10-30T22:49:56.831951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images = images_test[:2368] \ntest_data = dataset_fid(test_images,test_images,transform=transfor)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T19:41:30.719395Z","iopub.execute_input":"2021-09-12T19:41:30.719715Z","iopub.status.idle":"2021-09-12T19:41:30.762389Z","shell.execute_reply.started":"2021-09-12T19:41:30.719683Z","shell.execute_reply":"2021-09-12T19:41:30.760992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n                                 transforms.Scale(32),\n                                 transforms.ToTensor(),\n                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n                             ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,transform=None):\n        self.images_paths = images_paths \n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        img_transformed = self.transform(img)   \n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:51:34.554483Z","iopub.execute_input":"2021-10-30T22:51:34.554801Z","iopub.status.idle":"2021-10-30T22:51:34.564267Z","shell.execute_reply.started":"2021-10-30T22:51:34.554772Z","shell.execute_reply":"2021-10-30T22:51:34.563052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_images = dataset(output_images,transform=train_transforms)","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:52:59.863881Z","iopub.execute_input":"2021-10-30T22:52:59.864244Z","iopub.status.idle":"2021-10-30T22:52:59.869072Z","shell.execute_reply.started":"2021-10-30T22:52:59.864213Z","shell.execute_reply":"2021-10-30T22:52:59.868068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom torchvision.models.inception import inception_v3\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef inception_score(imgs, cuda=True, batch_size=32, resize=True, splits=1):\n    \"\"\"Computes the inception score of the generated images imgs\n    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n    cuda -- whether or not to run on GPU\n    batch_size -- batch size for feeding into Inception v3\n    splits -- number of splits\n    \"\"\"\n    N = len(imgs)\n\n    assert batch_size > 0\n    assert N > batch_size\n\n    # Set up dtype\n    if cuda:\n        dtype = torch.cuda.FloatTensor\n    else:\n        if torch.cuda.is_available():\n            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n        dtype = torch.FloatTensor\n\n    # Set up dataloader\n    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n\n    # Load inception model\n    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n    inception_model.eval();\n    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        return F.softmax(x).data.cpu().numpy()\n\n    # Get predictions\n    preds = np.zeros((N, 1000))\n\n    for i, batch in enumerate(dataloader, 0):\n        batch = batch.type(dtype)\n        batchv = Variable(batch)\n        batch_size_i = batch.size()[0]\n\n        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n\n    # Now compute the mean kl-div\n    split_scores = []\n\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = []\n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            scores.append(entropy(pyx, py))\n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores), np.std(split_scores)\n\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nprint (\"Calculating Inception Score...\")\nprint (inception_score(generated_images, cuda=True, batch_size=32, resize=True, splits=10))","metadata":{"execution":{"iopub.status.busy":"2021-10-30T22:53:21.804624Z","iopub.execute_input":"2021-10-30T22:53:21.804971Z","iopub.status.idle":"2021-10-30T22:59:53.077278Z","shell.execute_reply.started":"2021-10-30T22:53:21.804941Z","shell.execute_reply":"2021-10-30T22:59:53.075825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import linalg\nfrom PIL import Image\nbatch_size = 32\nsave_dir = './Output/'\nfrech = []\nd = 0\nt = 0\n    \nfor i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n    for images,captions,cap_lens in test_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        with torch.no_grad():\n            last_hidden_states = model(captions.to(device))\n            sent_emb = last_hidden_states[0][:,-1,:].detach().to(device)\n            #sent_emb = sent_emb.transpose(0, 1).contiguous()\n            \n        #print(\"captions :\",captions.size())\n        #print(\"captions :\",captions[3])\n        #print(\"images :\",images.size())\n        #words_embs, sent_emb = text_encoder(captions.to(device),cap_lens.to(device),hidden)\n        #words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n        with torch.no_grad():\n            noise = torch.randn(batch_size, 100)\n            noise=noise.to(device)\n            fake_imgs = netG(noise,sent_emb)\n        fd = calculate_fretchet(images,fake_imgs,model_incp)\n        print(fd)\n        frech.append(fd)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-16T12:31:51.195779Z","iopub.execute_input":"2021-09-16T12:31:51.196129Z","iopub.status.idle":"2021-09-16T12:32:30.957821Z","shell.execute_reply.started":"2021-09-16T12:31:51.196096Z","shell.execute_reply":"2021-09-16T12:32:30.956129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r ./output.zip ./Output","metadata":{"execution":{"iopub.status.busy":"2021-09-04T19:20:08.832673Z","iopub.execute_input":"2021-09-04T19:20:08.833018Z","iopub.status.idle":"2021-09-04T19:20:09.51403Z","shell.execute_reply.started":"2021-09-04T19:20:08.832987Z","shell.execute_reply":"2021-09-04T19:20:09.51306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}