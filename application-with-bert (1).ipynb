{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1435929,"sourceType":"datasetVersion","datasetId":841289},{"sourceId":2198477,"sourceType":"datasetVersion","datasetId":1320157},{"sourceId":2202811,"sourceType":"datasetVersion","datasetId":1322780},{"sourceId":2557993,"sourceType":"datasetVersion","datasetId":1552055},{"sourceId":2883073,"sourceType":"datasetVersion","datasetId":1766060},{"sourceId":2884774,"sourceType":"datasetVersion","datasetId":1767140}],"dockerImageVersionId":30140,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"5\"\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport Levenshtein\nimport cv2\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch.nn.utils.rnn import pack_padded_sequence\nimport torchvision\nimport PIL.Image as Image\nfrom torchvision.transforms import ToTensor, ToPILImage\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\nimport torch\nfrom torch import nn\nfrom nltk.translate.bleu_score import corpus_bleu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2024-03-03T20:57:43.092770Z","iopub.status.idle":"2024-03-03T20:57:43.093097Z","shell.execute_reply.started":"2024-03-03T20:57:43.092936Z","shell.execute_reply":"2024-03-03T20:57:43.092953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" # Increase Batch size based on your hardware capacity, if you have powerful GPU try BS=128,256...\ntrain_path=r'../input/bird-species-classification-220-categories/Train'\ntest_path=r'../input/bird-species-classification-220-categories/Test'","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.094554Z","iopub.status.idle":"2024-03-03T20:57:43.094836Z","shell.execute_reply.started":"2024-03-03T20:57:43.094686Z","shell.execute_reply":"2024-03-03T20:57:43.094700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_class = {}\nimages_class = []\nwith os.scandir('../input/captions/text/') as entries:\n    for entry in entries:\n        \n       # with os.scandir(entry) as entries:\n       if os.path.isdir(entry):\n            caption_class[entry.name.split('.')[1]] = entry.name\n        #caption_class.append(entry.name.split('.')[1])\nwith os.scandir('../input/bird-species-classification-220-categories/Train') as entrie:\n    for entry in entrie:\n        images_class.append(entry.name)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.101628Z","iopub.execute_input":"2024-03-03T20:57:43.102176Z","iopub.status.idle":"2024-03-03T20:57:43.173864Z","shell.execute_reply.started":"2024-03-03T20:57:43.102137Z","shell.execute_reply":"2024-03-03T20:57:43.172581Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/1336832502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcaption_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimages_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/captions/text/'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"],"ename":"NameError","evalue":"name 'os' is not defined","output_type":"error"}]},{"cell_type":"code","source":"print(len(images_class))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.174481Z","iopub.status.idle":"2024-03-03T20:57:43.174786Z","shell.execute_reply.started":"2024-03-03T20:57:43.174618Z","shell.execute_reply":"2024-03-03T20:57:43.174641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>Put the image paths and caption paths into lists to facilitate their usage during the loading and processing of the dataset, especially for handling both train and test sets.</h3>","metadata":{}},{"cell_type":"code","source":"import cv2\npath_caption = '../input/captions/text/'\n\ntrain_path=r'../input/bird-species-classification-220-categories/Train'\ntest_path=r'../input/bird-species-classification-220-categories/Test'\nimages_paths = []\ncaptions_paths = []\nimages_test = []\ncaptions_test = []\nname_captin = []\ntest_cap = []\n#embedding_paths = []\nfor c_image in images_class:\n    for image_name in os.listdir(os.path.join(train_path,c_image)):\n        d=os.path.join(train_path,c_image)\n        image_path =os.path.join(d,image_name)\n        images_paths.append(image_path)\n        name_captin_file = image_name[:-4]+\".txt\"\n        d=os.path.join(path_caption,caption_class[c_image])\n        caption_path =os.path.join(d,name_captin_file)\n        captions_paths.append(caption_path)\nfor c_image in images_class:\n    for image_name in os.listdir(os.path.join(test_path,c_image)):\n        d=os.path.join(test_path,c_image)\n        image_path =os.path.join(d,image_name)\n        images_test.append(image_path)\n        #img = cv2.imread(image_path)\n        name_captin_file = image_name[:-4]+\".txt\"\n        d=os.path.join(path_caption,caption_class[c_image])\n        caption_path =os.path.join(d,name_captin_file)\n       # embedding_path = os.path.join(d,embedding_caption)\n        captions_test.append(caption_path)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.176373Z","iopub.status.idle":"2024-03-03T20:57:43.176687Z","shell.execute_reply.started":"2024-03-03T20:57:43.176517Z","shell.execute_reply":"2024-03-03T20:57:43.176541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(images_paths),len(captions_paths))\nprint(len(images_test),len(captions_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.177551Z","iopub.status.idle":"2024-03-03T20:57:43.177881Z","shell.execute_reply.started":"2024-03-03T20:57:43.177688Z","shell.execute_reply":"2024-03-03T20:57:43.177712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2>Upload the pre-trained BERT and his tokenizer</H2>","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\n# Mini:   asafaya/bert-mini-arabic\n# Medium: asafaya/bert-medium-arabic\n# Base:   asafaya/bert-base-arabic\n# Large:  asafaya/bert-large-arabic\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.179218Z","iopub.status.idle":"2024-03-03T20:57:43.179511Z","shell.execute_reply.started":"2024-03-03T20:57:43.179353Z","shell.execute_reply":"2024-03-03T20:57:43.179368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> Extract all the tokens of the dataset, including train and test sets </H2>","metadata":{}},{"cell_type":"code","source":"toknaz = []\ns = captions_paths + captions_test\nfor i in s:\n    with open(i,'r') as file:\n        lines = file.read()\n        m = tokenizer.encode(lines)\n        for s in m:\n            if s not in toknaz:\n                toknaz.append(s)\nprint(len(toknaz)) ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.180607Z","iopub.status.idle":"2024-03-03T20:57:43.180949Z","shell.execute_reply.started":"2024-03-03T20:57:43.180744Z","shell.execute_reply":"2024-03-03T20:57:43.180767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.264261Z","iopub.execute_input":"2024-03-03T20:57:43.264451Z","iopub.status.idle":"2024-03-03T20:57:43.277244Z","shell.execute_reply.started":"2024-03-03T20:57:43.264428Z","shell.execute_reply":"2024-03-03T20:57:43.276250Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/1977153762.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"<H2> \"Determine the maximum text length and use it as the sequences length.\" </H2>","metadata":{}},{"cell_type":"code","source":"s = captions_paths + captions_test\nmax_seq = 0\nfor i in s:\n    with open(i,'r') as file:\n        lines = file.read()\n        m = len(tokenizer.encode(lines))\n        if max_seq < m:\n            max_seq = m","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.278028Z","iopub.status.idle":"2024-03-03T20:57:43.278342Z","shell.execute_reply.started":"2024-03-03T20:57:43.278169Z","shell.execute_reply":"2024-03-03T20:57:43.278193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.280463Z","iopub.status.idle":"2024-03-03T20:57:43.280897Z","shell.execute_reply.started":"2024-03-03T20:57:43.280650Z","shell.execute_reply":"2024-03-03T20:57:43.280673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> Clean the text, and find the maxumim langth of the text </H2>","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import RegexpTokenizer\ntokens_new = []\ns = captions_paths + captions_test\nmax_ = 0\nfor i in s:\n    lis = []\n    with open(i, \"r\") as f:\n        captions = f.read().split('\\n')\n    for cap in captions:\n        cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n        tokenizer = RegexpTokenizer(r'\\w+')\n        tokens = tokenizer.tokenize(cap.lower())\n        \n        for t in tokens:\n            lis.append(t)\n            t = t.encode('ascii', 'ignore').decode('ascii')\n            if t not in tokens_new:\n                tokens_new.append(t)\n    if max_ < len(lis):\n        max_ = len(lis)\n        \nprint(len(tokens_new))\nprint(max_)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.282356Z","iopub.status.idle":"2024-03-03T20:57:43.282655Z","shell.execute_reply.started":"2024-03-03T20:57:43.282501Z","shell.execute_reply":"2024-03-03T20:57:43.282517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> Add the end token</H2>","metadata":{}},{"cell_type":"code","source":"ixtoword = {}\nixtoword[0] = '<end>'\nwordtoix = {}\nwordtoix['<end>'] = 0\nix = 1\nfor w in tokens_new:\n    wordtoix[w] = ix\n    ixtoword[ix] = w\n    ix += 1","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.284001Z","iopub.status.idle":"2024-03-03T20:57:43.284412Z","shell.execute_reply.started":"2024-03-03T20:57:43.284183Z","shell.execute_reply":"2024-03-03T20:57:43.284209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> The pre-proccesssing of iamges and thier captions </H2>","metadata":{}},{"cell_type":"code","source":"def read_text(path):\n    tokens_new = []\n    with open(path, \"r\") as f:\n        captions = f.read().split('\\n')\n    for cap in captions:\n        cap = cap.replace(\"\\ufffd\\ufffd\", \" \")\n        tokenizer = RegexpTokenizer(r'\\w+')\n        tokens = tokenizer.tokenize(cap.lower())\n        \n        for t in tokens:\n            lis.append(t)\n            t = t.encode('ascii', 'ignore').decode('ascii')\n            if t not in tokens_new:\n                tokens_new.append(t)\n    return tokens_new\n\ndef text_to_tensor1(path,max_len):\n    seq_of_words = read_text(path)\n    m = [wordtoix[w] for w in tokens_new]\n    len_seq = len(m)\n    tensor = torch.LongTensor(F.pad(torch.LongTensor(m), pad=(0, (max_len+1 )- len_seq) , mode='constant', value=0)) \n    return tensor,len_seq\n\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(256),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.5], [0.5]),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\ntest_transforms =  transforms.Compose([\n        transforms.Resize((256,256)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\nclass dataset1(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,captions_paths,max_len,transform=None):\n        self.images_paths = images_paths\n        self.captions_paths = captions_paths\n        self.max_len = max_len \n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        tensor_caption,len_seq = text_to_tensor1(self.captions_paths[idx],self.max_len)\n        #seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long(\n        img_transformed = self.transform(img)   \n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed,tensor_caption,len_seq","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.287474Z","iopub.execute_input":"2024-03-03T20:57:43.287725Z","iopub.status.idle":"2024-03-03T20:57:43.314897Z","shell.execute_reply.started":"2024-03-03T20:57:43.287691Z","shell.execute_reply":"2024-03-03T20:57:43.313999Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/3229804767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m train_transforms =  transforms.Compose([\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"],"ename":"NameError","evalue":"name 'transforms' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def text_to_tensor(path,max_len):\n    with open(path,'r') as file:\n        lines = file.read()\n        m = tokenizer.encode(lines)\n        m.append(0)\n        len_seq = len(m)\n        tensor = torch.LongTensor(F.pad(torch.LongTensor(m), pad=(0, (max_len+1 )- len_seq) , mode='constant', value=0)) \n    return tensor,len_seq\n\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomCrop(256),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.5], [0.5]),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\ntest_transforms =  transforms.Compose([\n        transforms.Resize((256,256)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,captions_paths,max_len,transform=None):\n        self.images_paths = images_paths\n        self.captions_paths = captions_paths\n        self.max_len = max_len \n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        tensor_caption,len_seq = text_to_tensor(self.captions_paths[idx],self.max_len)\n        #seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long(\n        img_transformed = self.transform(img)   \n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed,tensor_caption,len_seq","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.315872Z","iopub.status.idle":"2024-03-03T20:57:43.316291Z","shell.execute_reply.started":"2024-03-03T20:57:43.316062Z","shell.execute_reply":"2024-03-03T20:57:43.316085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(images_paths),len(captions_paths))\nprint(len(images_test),len(captions_test))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.317755Z","iopub.status.idle":"2024-03-03T20:57:43.318156Z","shell.execute_reply.started":"2024-03-03T20:57:43.317962Z","shell.execute_reply":"2024-03-03T20:57:43.317989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq=300\nimages_paths  = images_paths[:9408]\ncaptions_paths = captions_paths[:9408]\ntest_images = images_test[:2368] \ntest_captions = captions_test[:2368] \ntrain_data = dataset(images_paths,captions_paths,max_seq,transform=train_transforms)\ntrain_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=32, shuffle=True)\ntest_data = dataset(test_images,test_captions,max_seq,transform=test_transforms)\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size=32, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.319877Z","iopub.status.idle":"2024-03-03T20:57:43.320258Z","shell.execute_reply.started":"2024-03-03T20:57:43.320085Z","shell.execute_reply":"2024-03-03T20:57:43.320106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data))\nprint(len(train_loader))\nprint(len(test_data))\nprint(len(test_loader))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.321502Z","iopub.status.idle":"2024-03-03T20:57:43.321804Z","shell.execute_reply.started":"2024-03-03T20:57:43.321637Z","shell.execute_reply":"2024-03-03T20:57:43.321651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> Define the GAN generator and discriminator</H2>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\n\nclass NetG(nn.Module):\n    def __init__(self, ngf=64, nz=100):\n        super(NetG, self).__init__()\n        self.ngf = ngf\n        self.cr = nn.Linear(768,256)\n        self.fc = nn.Linear(nz, ngf*8*4*4)\n        self.block0 = G_Block(ngf * 8, ngf * 8)#4x4\n        self.block1 = G_Block(ngf * 8, ngf * 8)#4x4\n        self.block2 = G_Block(ngf * 8, ngf * 8)#8x8\n        self.block3 = G_Block(ngf * 8, ngf * 8)#16x16\n        self.block4 = G_Block(ngf * 8, ngf * 4)#32x32\n        self.block5 = G_Block(ngf * 4, ngf * 2)#64x64\n        self.block6 = G_Block(ngf * 2, ngf * 1)#128x128\n\n        self.conv_img = nn.Sequential(\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ngf, 3, 3, 1, 1),\n            nn.Tanh(),\n        )\n\n    def forward(self, x, c):\n\n        out = self.fc(x)\n        out = out.view(x.size(0), 8*self.ngf, 4, 4)\n        c = self.cr(c)\n        out = self.block0(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block1(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block2(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block3(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block4(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block5(out,c)\n\n        out = F.interpolate(out, scale_factor=2)\n        out = self.block6(out,c)\n\n        out = self.conv_img(out)\n\n        return out\n\n\n\nclass G_Block(nn.Module):\n\n    def __init__(self, in_ch, out_ch):\n        super(G_Block, self).__init__()\n\n        self.learnable_sc = in_ch != out_ch \n        self.c1 = nn.Conv2d(in_ch, out_ch, 3, 1, 1)\n        self.c2 = nn.Conv2d(out_ch, out_ch, 3, 1, 1)\n        self.affine0 = affine(in_ch)\n        self.affine1 = affine(in_ch)\n        self.affine2 = affine(out_ch)\n        self.affine3 = affine(out_ch)\n        self.gamma = nn.Parameter(torch.zeros(1))\n        if self.learnable_sc:\n            self.c_sc = nn.Conv2d(in_ch,out_ch, 1, stride=1, padding=0)\n\n    def forward(self, x, y=None):\n        return self.shortcut(x) + self.gamma * self.residual(x, y)\n\n    def shortcut(self, x):\n        if self.learnable_sc:\n            x = self.c_sc(x)\n        return x\n\n    def residual(self, x, y=None):\n        h = self.affine0(x, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.affine1(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.c1(h)\n        \n        h = self.affine2(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        h = self.affine3(h, y)\n        h = nn.LeakyReLU(0.2,inplace=True)(h)\n        return self.c2(h)\n\n\n\nclass affine(nn.Module):\n\n    def __init__(self, num_features):\n        super(affine, self).__init__()\n\n        self.fc_gamma = nn.Sequential(OrderedDict([\n            ('linear1',nn.Linear(256, 256)),\n            ('relu1',nn.ReLU(inplace=True)),\n            ('linear2',nn.Linear(256, num_features)),\n            ]))\n        self.fc_beta = nn.Sequential(OrderedDict([\n            ('linear1',nn.Linear(256, 256)),\n            ('relu1',nn.ReLU(inplace=True)),\n            ('linear2',nn.Linear(256, num_features)),\n            ]))\n        self._initialize()\n\n    def _initialize(self):\n        nn.init.zeros_(self.fc_gamma.linear2.weight.data)\n        nn.init.ones_(self.fc_gamma.linear2.bias.data)\n        nn.init.zeros_(self.fc_beta.linear2.weight.data)\n        nn.init.zeros_(self.fc_beta.linear2.bias.data)\n\n    def forward(self, x, y=None):\n\n        weight = self.fc_gamma(y)\n        bias = self.fc_beta(y)        \n\n        if weight.dim() == 1:\n            weight = weight.unsqueeze(0)\n        if bias.dim() == 1:\n            bias = bias.unsqueeze(0)\n\n        size = x.size()\n        weight = weight.unsqueeze(-1).unsqueeze(-1).expand(size)\n        bias = bias.unsqueeze(-1).unsqueeze(-1).expand(size)\n        return weight * x + bias\n\n\nclass D_GET_LOGITS(nn.Module):\n    def __init__(self, ndf):\n        super(D_GET_LOGITS, self).__init__()\n        self.df_dim = ndf\n        self.kl = nn.Linear(768,256)\n        self.joint_conv = nn.Sequential(\n            nn.Conv2d(ndf * 16+256, ndf * 2, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2,inplace=True),\n            nn.Conv2d(ndf * 2, 1, 4, 1, 0, bias=False),\n        )\n\n    def forward(self, out, y):\n        y = self.kl(y)\n        y = y.view(-1, 256, 1, 1)\n        y = y.repeat(1, 1, 4, 4)\n        h_c_code = torch.cat((out, y), 1)\n        out = self.joint_conv(h_c_code)\n        return out\n\n\n\n\n\nclass NetD(nn.Module):\n    def __init__(self, ndf):\n        super(NetD, self).__init__()\n        self.conv_img = nn.Conv2d(3, ndf, 3, 1, 1)#128\n        self.block0 = resD(ndf * 1, ndf * 2)#64\n        self.block1 = resD(ndf * 2, ndf * 4)#32\n        self.block2 = resD(ndf * 4, ndf * 8)#16\n        self.block3 = resD(ndf * 8, ndf * 16)#8\n        self.block4 = resD(ndf * 16, ndf * 16)#4\n        self.block5 = resD(ndf * 16, ndf * 16)#4\n\n        self.COND_DNET = D_GET_LOGITS(ndf)\n\n    def forward(self,x):\n\n        out = self.conv_img(x)\n        out = self.block0(out)\n        out = self.block1(out)\n        out = self.block2(out)\n        out = self.block3(out)\n        out = self.block4(out)\n        out = self.block5(out)\n\n        return out\n\n\n\n\nclass resD(nn.Module):\n    def __init__(self, fin, fout, downsample=True):\n        super().__init__()\n        self.downsample = downsample\n        self.learned_shortcut = (fin != fout)\n        self.conv_r = nn.Sequential(\n            nn.Conv2d(fin, fout, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(fout, fout, 3, 1, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        self.conv_s = nn.Conv2d(fin,fout, 1, stride=1, padding=0)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x, c=None):\n        return self.shortcut(x)+self.gamma*self.residual(x)\n\n    def shortcut(self, x):\n        if self.learned_shortcut:\n            x = self.conv_s(x)\n        if self.downsample:\n            return F.avg_pool2d(x, 2)\n        return x\n\n    def residual(self, x):\n        return self.conv_r(x)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.323161Z","iopub.status.idle":"2024-03-03T20:57:43.323513Z","shell.execute_reply.started":"2024-03-03T20:57:43.323320Z","shell.execute_reply":"2024-03-03T20:57:43.323339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### device = \"cuda\"\nimport torchvision.utils as vutils\nimport random\nbatch_size = 32\nrandom.seed(100)\nnp.random.seed(100)\ntorch.manual_seed(100)\ntorch.cuda.manual_seed_all(100)\nCUDA_LAUNCH_BLOCKING=1\nnetG = NetG(batch_size, 100).to(device)\nnetD = NetD(batch_size).to(device)\noptimizerG = torch.optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\noptimizerD = torch.optim.Adam(netD.parameters(), lr=0.0004, betas=(0.0, 0.9))\ncheckpoint_G = torch.load('../input/modul-bert-490/netG_vBE490.pt')\nnetG.load_state_dict(checkpoint_G['model_state_dict'])\noptimizerG.load_state_dict(checkpoint_G['optimizer_state_dict'])\nepoch_ = checkpoint_G['epoch']\n\ncheckpoint_D = torch.load('../input/modul-bert-490/netD_vBE490.pt')\nnetD.load_state_dict(checkpoint_D['model_state_dict'])\noptimizerD.load_state_dict(checkpoint_D['optimizer_state_dict'])\nnetG.train()\nnetD.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.324971Z","iopub.status.idle":"2024-03-03T20:57:43.325379Z","shell.execute_reply.started":"2024-03-03T20:57:43.325156Z","shell.execute_reply":"2024-03-03T20:57:43.325177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> The Training loop of the GAN</H2>","metadata":{}},{"cell_type":"code","source":"#from miscc.utils import mkdir_p\n#from miscc.config import cfg, cfg_from_file\n\nfor epoch in range(epoch_ +1,epoch_ +41):\n    i = 0\n    for images,captions,cap_lens in train_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        #print(images.size())\n        with torch.no_grad():\n            last_hidden_states = model(captions.to(device))\n            sent_emb = last_hidden_states[0][:,-1,:].detach().to(device)\n        noise = torch.randn(batch_size, 100)\n        noise=noise.to(device)\n        fake = netG(noise,sent_emb.to(device))  \n            #imgs=images[0].to(device)\n        real_features = netD(images.to(device))\n        output = netD.COND_DNET(real_features,sent_emb.to(device))\n        errD_real = torch.nn.ReLU()(1.0 - output).mean()\n        output = netD.COND_DNET(real_features[:(batch_size - 1)].to(device), sent_emb[1:batch_size].to(device))\n        errD_mismatch = torch.nn.ReLU()(1.0 + output).mean()\n            #print(\"errD_mismatch :\",errD_mismatch.size())\n            # synthesize fake images\n        fake_features = netD(fake.detach()) \n        errD_fake = netD.COND_DNET(fake_features.to(device),sent_emb.to(device))\n        errD_fake = torch.nn.ReLU()(1.0 + errD_fake).mean()          \n        errD = errD_real + (errD_fake + errD_mismatch)/2.0\n            #print(\"errD : \",errD)\n        optimizerD.zero_grad()\n        optimizerG.zero_grad()\n        errD.backward()\n        optimizerD.step()\n            #MA-GP\n        interpolated = (images.data).requires_grad_()\n        sent_inter = (sent_emb.data).requires_grad_()\n        features = netD(interpolated.to(device))\n        out = netD.COND_DNET(features,sent_inter)\n        grads = torch.autograd.grad(outputs=out,\n                                    inputs=(interpolated,sent_inter),\n                                    grad_outputs=torch.ones(out.size()).to(device),\n                                    retain_graph=True,\n                                    create_graph=True,\n                                    only_inputs=True)\n        grad0 = grads[0].view(grads[0].size(0), -1).to(device)\n        grad1 = grads[1].view(grads[1].size(0), -1).to(device)\n        grad = torch.cat((grad0,grad1),dim=1)                        \n        grad_l2norm = torch.sqrt(torch.sum(grad ** 2, dim=1))\n        d_loss_gp = torch.mean((grad_l2norm) ** 6)\n        d_loss = 2.0 * d_loss_gp\n        optimizerD.zero_grad()\n        optimizerG.zero_grad()\n        d_loss.backward()\n        optimizerD.step()\n        i += 1  \n            # update G\n        features = netD(fake)\n        output = netD.COND_DNET(features,sent_emb)\n        errG = - output.mean()\n        optimizerG.zero_grad()\n        optimizerD.zero_grad()\n        errG.backward()\n        optimizerG.step()\n        print('[%d/%d][%d/%d] Loss_D: %.3f Loss_G %.3f'\n                % (epoch, 600, i, len(train_loader), errD.item(), errG.item()))\n        if (epoch+1) % 2 == 0 and i == 294:\n            print(\"save ................\")\n            vutils.save_image(fake.data,\n                        '%s/fake_samples_epoch_%03d.png' % ('./', epoch+1),\n                        normalize=True)\n            torch.save({\n            'model_state_dict': netD.state_dict(),\n            'optimizer_state_dict': optimizerD.state_dict(),\n            },'./netD_vBE520.pt')\n            \n            torch.save({\n            'epoch': epoch,\n            'model_state_dict': netG.state_dict(),\n            'optimizer_state_dict': optimizerG.state_dict(),\n            },'./netG_vBE520.pt')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir('./Output/')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> The evaluation of the model, the model take the text and generate the images</H2>","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:48.291231Z","iopub.execute_input":"2024-03-03T20:57:48.291896Z","iopub.status.idle":"2024-03-03T20:57:48.296944Z","shell.execute_reply.started":"2024-03-03T20:57:48.291841Z","shell.execute_reply":"2024-03-03T20:57:48.295820Z"}}},{"cell_type":"code","source":"#os.mkdir('./Output/')\nbatch_size = 32\nsave_dir = './Output/'\nd = 0\nt = 0\nfor i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n    for images,captions,cap_lens in test_loader:\n        cap_lens,index = torch.sort(cap_lens,descending = True)\n        captions = captions[index]\n        images = images[index]\n        with torch.no_grad():\n            last_hidden_states = model(captions.to(device))\n            sent_emb = last_hidden_states[0][:,-1,:].detach().to(device)\n            #sent_emb = sent_emb.transpose(0, 1).contiguous()\n            \n        #print(\"captions :\",captions.size())\n        #print(\"captions :\",captions[3])\n        #print(\"images :\",images.size())\n        #words_embs, sent_emb = text_encoder(captions.to(device),cap_lens.to(device),hidden)\n        #words_embs, sent_emb = words_embs.detach().to(device), sent_emb.detach().to(device)\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n        with torch.no_grad():\n            noise = torch.randn(batch_size, 100)\n            noise=noise.to(device)\n            fake_imgs = netG(noise,sent_emb)\n        print(\">>>>>>>>batch : \",d)\n        d += 1\n        for j in range(batch_size):\n            s_tmp = './Output/'\n            im = fake_imgs[j].data.cpu().numpy()\n                # [-1, 1] --> [0, 255]\n            arab_cap = captions[j]\n            im = (im + 1.0) * 127.5\n            im = im.astype(np.uint8)\n            im = np.transpose(im, (1, 2, 0))\n            im = Image.fromarray(im)\n            arab_cap = tokenizer.decode(arab_cap)\n            cap_path = '%s_%3d.txt' % (s_tmp,t)\n            fullpath = '%s_%3d.png' % (s_tmp,t)\n            t += 1\n            im.save(fullpath)\n            #with open(cap_path,'w') as f:\n             #   f.write(arab_cap)\n                ","metadata":{"execution":{"iopub.status.busy":"2022-01-03T23:32:15.283878Z","iopub.execute_input":"2022-01-03T23:32:15.284367Z","iopub.status.idle":"2022-01-03T23:34:39.568705Z","shell.execute_reply.started":"2022-01-03T23:32:15.284326Z","shell.execute_reply":"2022-01-03T23:34:39.567977Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r Output_bert.zip ./","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.359833Z","iopub.status.idle":"2024-03-03T20:57:43.360235Z","shell.execute_reply.started":"2024-03-03T20:57:43.360062Z","shell.execute_reply":"2024-03-03T20:57:43.360086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<H2> The rest of code is about the evaluation using FID and IS</H2>","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\nclass InceptionV3(nn.Module):\n    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,   # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False):\n        \"\"\"Build pretrained InceptionV3\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, normalizes the input to the statistics the pretrained\n            Inception network expects\n        requires_grad : bool\n            If true, parameters of the model require gradient. Possibly useful\n            for finetuning the network\n        \"\"\"\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            'Last possible output block index is 3'\n\n        self.blocks = nn.ModuleList()\n\n        inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        \"\"\"Get Inception feature maps\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in \n            range (0, 1)\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output \n        block, sorted ascending by index\n        \"\"\"\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.upsample(x, size=(299, 299), mode='bilinear', align_corners=True)\n\n        if self.normalize_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:43.449978Z","iopub.execute_input":"2024-03-03T20:57:43.450173Z","iopub.status.idle":"2024-03-03T20:57:45.164158Z","shell.execute_reply.started":"2024-03-03T20:57:43.450151Z","shell.execute_reply":"2024-03-03T20:57:45.163443Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\nmodel_incp = InceptionV3([block_idx])\nmodel_incp=model_incp.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:45.165701Z","iopub.execute_input":"2024-03-03T20:57:45.165949Z","iopub.status.idle":"2024-03-03T20:57:46.886905Z","shell.execute_reply.started":"2024-03-03T20:57:45.165920Z","shell.execute_reply":"2024-03-03T20:57:46.885423Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/104M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04ab431a7e1e4fb4bc8a15f86bb2ef3e"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_20/507933514.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mblock_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBLOCK_INDEX_BY_DIM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_incp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel_incp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_incp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"],"ename":"NameError","evalue":"name 'device' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n                    cuda=False):\n    model.eval()\n    act=np.empty((len(images), dims))\n    \n    if cuda:\n        batch=images.cuda()\n    else:\n        batch=images\n    pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n    if pred.size(2) != 1 or pred.size(3) != 1:\n        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n    \n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.888314Z","iopub.status.idle":"2024-03-03T20:57:46.888765Z","shell.execute_reply.started":"2024-03-03T20:57:46.888525Z","shell.execute_reply":"2024-03-03T20:57:46.888549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    \"\"\"Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n    \"\"\"\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        'Training and test mean vectors have different lengths'\n    assert sigma1.shape == sigma2.shape, \\\n        'Training and test covariances have different dimensions'\n\n    diff = mu1 - mu2\n\n    \n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    if not np.isfinite(covmean).all():\n        msg = ('fid calculation produces singular product; '\n               'adding %s to diagonal of cov estimates') % eps\n        print(msg)\n        offset = np.eye(sigma1.shape[0]) * eps\n        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n    \n    if np.iscomplexobj(covmean):\n        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n            m = np.max(np.abs(covmean.imag))\n            raise ValueError('Imaginary component {}'.format(m))\n        covmean = covmean.real\n\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) +\n            np.trace(sigma2) - 2 * tr_covmean)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.889939Z","iopub.status.idle":"2024-03-03T20:57:46.890370Z","shell.execute_reply.started":"2024-03-03T20:57:46.890146Z","shell.execute_reply":"2024-03-03T20:57:46.890170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_fretchet(images_real,images_fake,model):\n    mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n    mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n    \n    #\"\"\"get fretched distance\"\"\"\n    fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n    return fid_value","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.891716Z","iopub.status.idle":"2024-03-03T20:57:46.892049Z","shell.execute_reply.started":"2024-03-03T20:57:46.891877Z","shell.execute_reply":"2024-03-03T20:57:46.891898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n    \n        \ntrain_transforms =  transforms.Compose([\n        transforms.Resize(299),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                             ])\n\nclass dataset(torch.utils.data.Dataset):\n\n    def __init__(self,images_paths,transform=None):\n        self.images_paths = images_paths \n        self.transform = transform \n         \n    #dataset length\n    def __len__(self):\n        self.filelength = len(self.images_paths)\n        return self.filelength\n    \n    #load an one of images\n    def __getitem__(self,idx):\n     #   img_path = self.file_list[idx]\n        img = pil_loader(self.images_paths[idx].strip())\n        img_transformed = self.transform(img)   \n        #img_transformed = img_transformed.expand(3,256,256)\n        return img_transformed","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.893589Z","iopub.status.idle":"2024-03-03T20:57:46.894033Z","shell.execute_reply.started":"2024-03-03T20:57:46.893777Z","shell.execute_reply":"2024-03-03T20:57:46.893799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"./Output/\"\noutput_images = []\nwith os.scandir('./Output') as entrie:\n    for entry in entrie:\n        output_images.append(path+entry.name)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.895654Z","iopub.status.idle":"2024-03-03T20:57:46.896097Z","shell.execute_reply.started":"2024-03-03T20:57:46.895846Z","shell.execute_reply":"2024-03-03T20:57:46.895882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_images = dataset(output_images,transform=train_transforms)","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.897374Z","iopub.status.idle":"2024-03-03T20:57:46.897785Z","shell.execute_reply.started":"2024-03-03T20:57:46.897562Z","shell.execute_reply":"2024-03-03T20:57:46.897584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom torchvision.models.inception import inception_v3\n\nimport numpy as np\nfrom scipy.stats import entropy\n\ndef inception_score(imgs, cuda=True, batch_size=32, resize=True, splits=1):\n    \"\"\"Computes the inception score of the generated images imgs\n    imgs -- Torch dataset of (3xHxW) numpy images normalized in the range [-1, 1]\n    cuda -- whether or not to run on GPU\n    batch_size -- batch size for feeding into Inception v3\n    splits -- number of splits\n    \"\"\"\n    N = len(imgs)\n\n    assert batch_size > 0\n    assert N > batch_size\n\n    # Set up dtype\n    if cuda:\n        dtype = torch.cuda.FloatTensor\n    else:\n        if torch.cuda.is_available():\n            print(\"WARNING: You have a CUDA device, so you should probably set cuda=True\")\n        dtype = torch.FloatTensor\n\n    # Set up dataloader\n    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n\n    # Load inception model\n    inception_model = inception_v3(pretrained=True, transform_input=False).type(dtype)\n    inception_model.eval();\n    up = nn.Upsample(size=(299, 299), mode='bilinear').type(dtype)\n    def get_pred(x):\n        if resize:\n            x = up(x)\n        x = inception_model(x)\n        return F.softmax(x).data.cpu().numpy()\n\n    # Get predictions\n    preds = np.zeros((N, 1000))\n\n    for i, batch in enumerate(dataloader, 0):\n        batch = batch.type(dtype)\n        batchv = Variable(batch)\n        batch_size_i = batch.size()[0]\n\n        preds[i*batch_size:i*batch_size + batch_size_i] = get_pred(batchv)\n\n    # Now compute the mean kl-div\n    split_scores = []\n\n    for k in range(splits):\n        part = preds[k * (N // splits): (k+1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        scores = []\n        for i in range(part.shape[0]):\n            pyx = part[i, :]\n            scores.append(entropy(pyx, py))\n        split_scores.append(np.exp(np.mean(scores)))\n\n    return np.mean(split_scores), np.std(split_scores)\n\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nprint (\"Calculating Inception Score...\")\nprint (inception_score(generated_images, cuda=True, batch_size=32, resize=True, splits=10))","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.899373Z","iopub.status.idle":"2024-03-03T20:57:46.899795Z","shell.execute_reply.started":"2024-03-03T20:57:46.899561Z","shell.execute_reply":"2024-03-03T20:57:46.899583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy import linalg\nfrom PIL import Image\nbatch_size = 32\nsave_dir = './Output/'\nfrech = []\nd = 0\nt = 0\n    \nfor i in range(1):  # (cfg.TEXT.CAPTIONS_PER_IMAGE):\n    for images,captions,cap_lens in test_loader:\n        #cap_lens,index = torch.sort(cap_lens,descending = True)\n        #captions = captions[index]\n        #images = images[index]\n        with torch.no_grad():\n            #last_hidden_states = model(captions.to(device))\n            #sent_emb = last_hidden_states[0][:,-1,:].detach().to(device)\n            #sent_emb = sent_emb.transpose(0, 1).contiguous()\n            \n            hidden = text_encoder.init_hidden(batch_size)\n            # words_embs: batch_size x nef x seq_len\n            # sent_emb: batch_size x nef\n            words_embs, sent_emb = text_encoder(captions.to(device), cap_lens.to(device), hidden)\n            words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n            #######################################################\n            # (2) Generate fake images\n            ######################################################\n        with torch.no_grad():\n            noise = torch.randn(batch_size, 100)\n            noise=noise.to(device)\n            fake_imgs = netG(noise,sent_emb)\n        fd = calculate_fretchet(images,fake_imgs,model_incp)\n        print(fd)\n        frech.append(fd)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-03-03T20:57:46.901053Z","iopub.status.idle":"2024-03-03T20:57:46.901370Z","shell.execute_reply.started":"2024-03-03T20:57:46.901198Z","shell.execute_reply":"2024-03-03T20:57:46.901222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}